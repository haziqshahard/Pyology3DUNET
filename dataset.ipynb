{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/rds/general/user/hhs21/home/anaconda3/envs/pyseghpc/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "  File \"/var/tmp/pbs.9223122.pbs/ipykernel_2785014/2927388279.py\", line 2, in <module>\n",
      "    from torch.utils.data import Dataset\n",
      "ModuleNotFoundError: No module named 'torch'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/rds/general/user/hhs21/home/anaconda3/envs/pyseghpc/lib/python3.11/site-packages/pygments/styles/__init__.py\", line 45, in get_style_by_name\n",
      "ModuleNotFoundError: No module named 'pygments.styles.default'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/rds/general/user/hhs21/home/anaconda3/envs/pyseghpc/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 2168, in showtraceback\n",
      "  File \"/rds/general/user/hhs21/home/anaconda3/envs/pyseghpc/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1454, in structured_traceback\n",
      "  File \"/rds/general/user/hhs21/home/anaconda3/envs/pyseghpc/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1345, in structured_traceback\n",
      "  File \"/rds/general/user/hhs21/home/anaconda3/envs/pyseghpc/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1192, in structured_traceback\n",
      "  File \"/rds/general/user/hhs21/home/anaconda3/envs/pyseghpc/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n",
      "  File \"/rds/general/user/hhs21/home/anaconda3/envs/pyseghpc/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1133, in get_records\n",
      "  File \"/rds/general/user/hhs21/home/anaconda3/envs/pyseghpc/lib/python3.11/site-packages/pygments/styles/__init__.py\", line 47, in get_style_by_name\n",
      "pygments.util.ClassNotFound: Could not find style module 'pygments.styles.default', though it should be builtin.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.autograd import Variable\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "def make_one_hot(labels, device, C=2):\n",
    "    '''\n",
    "    Converts integer labels to one-hot encoding for semantic segmentation tasks.\n",
    " \n",
    "    Parameters\n",
    "    ----------\n",
    "    labels : torch.autograd.Variable of torch.cuda.LongTensor\n",
    "        Shape: N x 1 x H x W, where N is the batch size.\n",
    "        Each value is an integer representing correct classification.\n",
    "    C : integer\n",
    "        Number of classes in labels.\n",
    " \n",
    "    Returns\n",
    "    -------\n",
    "    target : torch.autograd.Variable of torch.cuda.FloatTensor\n",
    "        Shape: N x C x H x W, where C is the class number. One-hot encoded.\n",
    "    '''\n",
    "    # Ensure labels are of type LongTensor\n",
    "    labels = labels.long().to(device)\n",
    "   \n",
    "    # Create a zero-initialized one-hot tensor with the appropriate dimensions\n",
    "    one_hot = torch.FloatTensor(C, labels.size(1), labels.size(2), labels.size(3)).zero_().to(device)\n",
    "   \n",
    "    # Use scatter_ to set the corresponding class index to 1 for each pixel\n",
    "    target = one_hot.scatter_(0, labels.data,1)\n",
    "   \n",
    "    # Convert the result to a torch.autograd.Variable\n",
    "    target = Variable(target)\n",
    "       \n",
    "    return target\n",
    "\n",
    "# Define a function to read NIfTI image from a given path\n",
    "def readNifti_img(path):\n",
    "    # Load the NIfTI image and normalize pixel values\n",
    "    # print(path)\n",
    "    img_ = nib.load(path).get_fdata()\n",
    "    img_ = img_ / img_.max()\n",
    "    # Convert to torch tensor and add channel dimension\n",
    "    img_ = torch.tensor(img_, dtype=torch.float32).unsqueeze(0)\n",
    "    return img_\n",
    "\n",
    "# Define a function to read NIfTI mask from a given path\n",
    "def readNifti_mask(path):\n",
    "    # Load the NIfTI mask and convert to torch tensor with channel dimension\n",
    "    # print(path)\n",
    "    mask_ = nib.load(path).get_fdata()\n",
    "    mask_ = torch.tensor(mask_, dtype=torch.float32).unsqueeze(0)\n",
    "    #mask_ = make_one_hot(mask_,torch.device('cuda'),3)\n",
    "    #Convert to categorical to be [1,3,112,112,112]\n",
    "    return mask_\n",
    "\n",
    "class NiftiDataset(Dataset):\n",
    "   def __init__(self, image_paths, mask_paths, transform=None):\n",
    "       # Initialize dataset with image and mask paths, fixed, moving, and transform\n",
    "       self.image_paths = image_paths\n",
    "       self.mask_paths = mask_paths\n",
    "       self.transform = transform\n",
    "\n",
    "\n",
    "   def __len__(self):\n",
    "       # Return the total number of samples in the dataset\n",
    "       return len(self.image_paths)\n",
    "\n",
    "   def __getitem__(self, idx):\n",
    "       # Get the head and tail of the image path\n",
    "           \n",
    "       train_img = readNifti_img(self.image_paths[idx])\n",
    "       #print(train_img.shape)\n",
    "       #print(train_img.dtype)\n",
    "       train_mask = readNifti_mask(self.mask_paths[idx])\n",
    "       #print(train_mask.shape)\n",
    "       #print(train_mask.dtype)\n",
    "       # subject = [train_img, train_mask, self.image_paths[idx]]\n",
    "\n",
    "       subject = {'train_img': train_img,\n",
    "                    'train_mask': train_mask,\n",
    "                    'name':self.image_paths[idx]}\n",
    "       \n",
    "       #Add name here to check\n",
    "       \n",
    "       if self.transform:\n",
    "           subject = self.transform(subject)\n",
    "\n",
    "       # Return the subject dictionary\n",
    "       return subject\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    return readNifti_mask(\"C:/Users/Haziq/OneDrive - Imperial College London/Third Year/Fetal VR Group Project/Files/3D 112 Dataset/test/mask/Case_122_Pre/Case_122_Pre_mask_time002.nii\")\n",
    "\n",
    "def voltosl(vol,sliceno):\n",
    "    #Volume is shape [1,1,112,112,112]\n",
    "    #Need to plot each slice at sliceno\n",
    "    #vol = vol[2,:,:,20].cpu()\n",
    "    vol=torch.argmax(vol,0).cpu()\n",
    "    vol = vol[:,:,20]\n",
    "    # print(vol)\n",
    "    slice = np.asarray(vol.detach().numpy())\n",
    "    slice = slice.reshape([112,112])\n",
    "    #print(slice.shape)\n",
    "    return slice\n",
    "    print(y.shape)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #import matplotlib.pyplot as plt\n",
    "    #plt.imshow(voltosl(main(),40))\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyseg]",
   "language": "python",
   "name": "conda-env-pyseg-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
